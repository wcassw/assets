

Ask These 5 Questions (in order)

1. Is it user-facing?
    * External customers?
    * Internal users only?
    * Background jobs?
3. How many services are affected?
    * One service?
    * A dependency chain?
    * Everything behind ingress?
4. How many environments?
    * Prod only?
    * Multiple clusters?
    * One region or all regions?
5. How much traffic is impacted?
    * 1 pod?
    * 1 deployment?
    * Entire node pool?
6. Is the impact growing or contained?
    * Stable
    * Degrading
    * Cascading


üö® Production Incident Flow (One Page)

Scope: Kubernetes + Linux
Audience: Primary & Secondary On-Call
Goal: Restore service fast, escalate correctly

1Ô∏è‚É£ PagerDuty Fires (0‚Äì2 min)

‚òë Acknowledge immediately
‚òë Post in incident channel:

Ack‚Äôd. Investigating.


If you can‚Äôt act now ‚Üí reassign or escalate.

2Ô∏è‚É£ Confirm Impact (2‚Äì5 min)

Ask:

User-facing or internal?

One service or many?

Recent deploy?

Quick check:

* kubectl get nodes
* kubectl get pods -A | grep -E 'Crash|Error|Pending'

3Ô∏è‚É£ Decide the Path (Critical Decision)
Signal	Go Here
CrashLoop / rollout / errors	Kubernetes
Pods Pending / Node NotReady	Linux
Widespread failures	Kubernetes ‚Üí Linux

Rule: Kubernetes first. SSH only if K8s points you there.

4Ô∏è‚É£ Kubernetes Path (Fast Fixes)
Deployments / Pods
kubectl describe pod <pod>
kubectl logs <pod> --previous
kubectl rollout undo deployment <name>


‚úî Roll back before debugging
‚ùå No shelling into pods during outages

Latency / Errors
kubectl top pods
kubectl get hpa


‚úî Scale replicas
‚úî Increase HPA max
‚úî Roll back recent deploy

Ingress / Traffic
* kubectl get pods -n ingress-nginx
* kubectl logs -n ingress-nginx <pod>


‚úî Restart ingress
‚úî Roll back ingress config
‚úî Fail over DNS if available

5Ô∏è‚É£ Linux Path (Only When Needed)
Node NotReady / Unreachable
kubectl describe node <node>


If SSH works:

* uptime
* journalctl -xb


‚úî Replace node if recovery >10 min

* CPU / Memory / Disk
* top
* free -h
* df -h


‚úî Restart offender
‚úî Clean safely
‚úî Replace node if unstable

Rule: A replaced node is better than a sick node.

6Ô∏è‚É£ Reboot / Replace Decision
* Condition	Action
* Kernel panic	Reboot
* Hung I/O	Reboot
* Disk full	Clean / Replace
* Repeated OOM	Replace
* Unknown + unstable	Replace
7Ô∏è‚É£ Escalate (‚â§15 min)

Escalate if:

Still broken after 15 min

Multiple nodes/services impacted

Control plane involved

You‚Äôre unsure

Targets:

Secondary on-call

Platform / Infra

Cloud provider

8Ô∏è‚É£ Verify Recovery

Must be true:

Error rate normal

Latency stable

Pods Ready

Nodes Ready

* kubectl get nodes
* kubectl get pods -A

9Ô∏è‚É£ Resolve & Communicate

PagerDuty:

Resolve incident

Slack:

Resolved. Service stable. RCA to follow.

üîí On-Call Rules (Memorize These)

Restore service first

* Rollback beats debugging

* Replace nodes without guilt

Escalate after 15 minutes

* Document everything
